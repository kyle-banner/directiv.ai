<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Directiv.ai | Blog</title>
    <style>
        :root {
            --bg: #0f1116;
            --panel: #141824;
            --text: #e4e6eb;
            --muted: #b9bcc3;
            --accent: #2b8eff;
            --accent-light: #5aa7ff;
            --font: "Inter", system-ui, -apple-system, sans-serif;
        }

        * {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            font-family: var(--font);
            background: radial-gradient(circle at top, #151a2a, var(--bg) 45%);
            color: var(--text);
            min-height: 100vh;
            padding: 2rem;
        }

        a {
            color: var(--accent-light);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
        }

        .nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 2.5rem;
        }

        .nav a {
            font-weight: 600;
            color: var(--text);
        }

        .title {
            margin: 0 0 0.5rem;
            font-size: 2.4rem;
            letter-spacing: -0.02em;
        }

        .subtitle {
            margin: 0 0 2rem;
            color: var(--muted);
            line-height: 1.6;
        }

        .post {
            background: var(--panel);
            border: 1px solid #1f2434;
            border-radius: 1rem;
            padding: 2rem;
            margin-bottom: 2rem;
            box-shadow: 0 18px 40px rgba(0, 0, 0, 0.25);
        }

        .post-title {
            margin: 0 0 0.2rem;
            font-size: 1.8rem;
        }

        .post-meta {
            font-size: 0.95rem;
            color: var(--muted);
            margin-bottom: 1.5rem;
        }

        .post-block {
            display: grid;
            grid-template-columns: minmax(0, 1fr);
            gap: 1.5rem;
            align-items: start;
        }

        .post-body p {
            margin: 0 0 1rem;
            line-height: 1.7;
            color: var(--text);
        }

        .post-body strong {
            color: #fff;
        }

        figure {
            margin: 0;
        }

        img {
            width: 100%;
            border-radius: 0.75rem;
            display: block;
            border: 1px solid #262c3f;
        }

        figcaption {
            margin-top: 0.6rem;
            margin-bottom: 0.6rem;
            font-size: 0.9rem;
            color: var(--muted);
        }

        figure a {
            display: block;
        }

        .video-embed {
            position: relative;
            padding-top: 56.25%;
            margin: 1.5rem 0;
            border-radius: 0.75rem;
            overflow: hidden;
            border: 1px solid #262c3f;
        }

        .video-embed iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            border: 0;
        }

        .cta-row {
            margin-top: 1.5rem;
            display: flex;
            flex-wrap: wrap;
            gap: 0.8rem;
        }

        .cta {
            display: inline-flex;
            align-items: center;
            gap: 0.4rem;
            padding: 0.7rem 1.2rem;
            border-radius: 999px;
            background: var(--accent);
            color: #fff;
            font-weight: 600;
            transition: background 0.2s ease;
        }

        .cta:hover {
            background: var(--accent-light);
        }

        @media (max-width: 800px) {
            body {
                padding: 1.5rem;
            }

            .post {
                padding: 1.5rem;
            }

            .post-block {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="nav">
            <a href="index.html">&lt;- Back to Directiv.ai</a>
            <a href="mailto:kyle@directiv.ai">Contact</a>
        </div>

        <h1 class="title">Directiv.ai Blog</h1>
        <p class="subtitle">
            Short field notes on agentic systems, deployments, and modernizing code-driven products
        </p>

        <!-- Duplicate this article block for each new post -->
        <article class="post">
            <h2 class="post-title">GenAI Isn't the Problem. Your Architecture Is.</h2>
            <div class="post-meta">January 22, 2026 - 8 min read</div>
            <div class="post-block">
                <div class="post-body">
                    <p>In <em>The Hitchhiker's Guide to the Galaxy</em>, the Infinite Improbability Drive is the core of
                        the Heart of Gold, the crew's interstellar spaceship. When activated, it routinely causes wildly
                        improbable and chaotic events, like turning missiles into a whale and a bowl of petunias. It's a
                        notoriously unreliable and dangerous technology that can change the physical state of the ship
                        and its crew. And yet, it's also the component that makes instantaneous, universe-bending jumps
                        possible. The Heart of Gold itself surrounds the Infinite Improbability Drive and provides
                        structure,
                        controls, and safety for its crew (most of the time).</p>

                    <div class="video-embed">
                        <iframe src="https://www.youtube.com/embed/BFSst3ujx6U" title="YouTube video player"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                    </div>

                    <p>In your AI-enabled applications, an LLM plays the same role as the drive. It generates powerful,
                        probabilistic answers at incredible speed. What the LLM cannot do is own state, enforce rules,
                        or
                        safely recover from failure on its own. That responsibility belongs to the rest of the
                        application. In this metaphor that's the spaceship. When an LLM is boxed inside a system with
                        clear execution rules and
                        control points, you end up with something that might just actually provide value in production.
                    </p>

                    <p>In 2026, most enterprise GenAI application failures are not model-quality problems. They're
                        control-plane problems. Organizations are dropping probabilistic systems into workflows that
                        were built to be predictable and auditable, without adding the architectural pieces those
                        workflows depend on.</p>

                    <p>If you want LLMs to play a core role in your systems, you need a clear separation of
                        responsibilities. The model should behave like a pure function inside a workflow engine that
                        owns execution, retries, and state transitions. Inputs, tool calls, outputs, and side effects
                        need to be versioned, replayable, and diffable.</p>

                    <p>An LLM should never be allowed to directly mutate enterprise systems. It should only propose
                        actions. A governing orchestration layer decides what is allowed, enforces policy and
                        idempotency, and applies evaluation gates before anything mutates real systems.</p>

                    <p>This is where policy engines like <a href="https://www.openpolicyagent.org/">Open Policy
                            Agent</a> fit cleanly. OPA gives you an explicit decision point:</p>
                    <ul>
                        <li>Is this action allowed?</li>
                        <li>Does it require approval?</li>
                        <li>What constraints apply?</li>
                    </ul>
                    <p>The model shouldn't answer those questions. Your architecture should.</p>

                    <p>Every input, tool call, policy decision, and output should be written to an append-only run
                        ledger so executions are replayable and auditable. Models may suggest actions, but the system
                        decides what actually happens and records why. Side effects occur only through controlled
                        executors with idempotency keys, so retries don't quietly duplicate work. Higher-risk actions
                        can be routed through a human approval queue as a deliberate control, not a safety blanket.</p>

                    <p> All of this might sound like overkill, and in some AI-enabled applications this might bot be
                        necessary. But the system around the LLM should be predictable where it matters for your use
                        case e.g. state changes, side
                        effects, and governance decisions. Once an LLM's output is captured as a
                        versioned input, the orchestration, policy, and execution layers will behave the same way every
                        time.</p>

                    <figure>
                        <a href="assets/blog/llm-in-deterministic-system.png" target="_blank" rel="noopener">
                            <img src="assets/blog/llm-in-deterministic-system.png"
                                alt="LLM inside a controlled orchestration system with policy and logging" />
                        </a>
                        <figcaption>Models propose. Systems decide, execute, and record.</figcaption>
                    </figure>

                    <p>And yes, it's tempting to say “we'll just route everything to a human.” That feels safe, but it
                        introduces its own failure modes. More on that in a future post.</p>

                    <p>GenAI feels wrong in many enterprises because it's being sprayed across workflows without
                        changing the underlying architecture. We're inserting probabilistic components into environments
                        that were designed for predictability, auditability, and control—and then pretending nothing
                        else needs to change.</p>

                    <p>This isn't an AI maturity issue. It's a systems design failure. AI-enabled apps feel wrong
                        because we're
                        treating AI like the application itself, when it should be treated more like infrastructure.</p>

                    <p>The companies getting this right aren't “using more AI.” They're redesigning the systems around
                        it. They're narrowing the use of AI to precise, value-adding features. They understand that
                        models can suggest actions, but the surrounding system must decide,
                        execute, and record.</p>

                    <p>Get this right and you're the Heart of Gold. Get it wrong and you're the bowl of petunias, with
                        just enough time for one last thought: “oh no, not again.”</p>

                    <p>If you find yourself thinking about these problems too, let's nerd out. Drop me a line anytime at
                        <a href="mailto:kyle@directiv.ai">kyle@directiv.ai</a>.
                    </p>

                </div>
            </div>
        </article>
    </div>
</body>

</html>